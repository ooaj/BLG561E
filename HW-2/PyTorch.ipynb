{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "Follow the tutorial about how to utilize Google Colab but **don't install PyTorch** as mentioned in the blog post.\n",
    "\n",
    "Turkish:\n",
    "https://medium.com/deep-learning-turkiye/google-colab-ile-%C3%BCcretsiz-gpu-kullan%C4%B1m%C4%B1-30fdb7dd822e\n",
    "\n",
    "English:\n",
    "https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Now, you will work on the problem of Food Classification using a subset of Food-101 data, where you have 50 classes \n",
    "\n",
    "After having mounted the Jupyter Notebook to Google Drive, navigate the following address:\n",
    "\n",
    "https://drive.google.com/open?id=13zG2cXkDZiSGZTALgZD09OP7HSF1twWV\n",
    "\n",
    "Add this folder entirely to your Google Drive. If you have done it correctly, then you should be able to see blg561 in your drive.\n",
    "\n",
    "Let's try it with an simple example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to choose the right runtime from the menu above. (GPU should be selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# This command should return some information about the GPU status if the runtime is right. \n",
    "# In addition to that, if you encounter memory issues, you can diagnose your model by this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r drive/blg561/requirements.txt\n",
    "# This code should install the required software if you did the integration correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to utilize Pytorch methods in this part of the Homework. \n",
    "You will be using pretained models VGG16, ResNet, Inception and your own model.\n",
    "\n",
    "Below, you will find required modules loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms.functional import normalize, resize, to_tensor\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader and Dataset\n",
    "Here, we provide you an almost fully functional Dataset but to ensure that you understood how it works, we request you to do a little adjustment. Try to understand how this loader code works and add Train/Test/Validation split to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_extension(id, extension): Image.EXTENSION[extension.lower()] = id.upper()\n",
    "Image.register_extension = register_extension\n",
    "def register_extensions(id, extensions): \n",
    "    for extension in extensions: register_extension(id, extension)\n",
    "Image.register_extensions = register_extensions\n",
    "\n",
    "### The code below is to make you be able to load by using Pillow.\n",
    "class Food101Loader(Dataset):\n",
    "    def __init__(self, path, mode='train'):\n",
    "        self.images_path = os.path.join(path, './')\n",
    "        self.image_names = {}\n",
    "        for i,d in enumerate(os.listdir(self.images_path)):\n",
    "            try:\n",
    "                d = os.path.join(self.images_path, d)\n",
    "                single_class_path = {os.path.join(d,im):i for im in os.listdir(d)}\n",
    "                self.image_names= {**self.image_names, **single_class_path}\n",
    "            except:\n",
    "                print('err')\n",
    "        \n",
    "        self.image_names = [el for el in self.image_names.items()]\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.image_names = None\n",
    "        if mode == 'test':\n",
    "            self.image_names = None\n",
    "        if mode == 'val':\n",
    "            self.image_names = None\n",
    "            \n",
    "    # This method normalizes each image individually. You may want to normalize it by using ImageNet statistics\n",
    "    # If you would like to fine tune so feel free to do changes\n",
    "    def preprocess(self, x):\n",
    "        x = resize(x, (224, 224))\n",
    "        x = np.asarray(x)\n",
    "        x = x.transpose((2,0,1))\n",
    "        x = torch.from_numpy(x).type(torch.float)\n",
    "        normalize(x,  mean=[el.mean() for el in x] ,std=[el.std() for el in x])\n",
    "        return x\n",
    "    \n",
    "    # This method basically loads image from the file system\n",
    "    def load(self, path):\n",
    "        img = Image.open(path)\n",
    "        img.load()\n",
    "        return img\n",
    "\n",
    "    # This method should be overrided in order to access the inside of dataset\n",
    "    def __getitem__(self, ix):\n",
    "        # Get its relative path and label\n",
    "        data_path, label = self.image_names[ix]\n",
    "        \n",
    "        # Load image\n",
    "        data = self.load(data_path)\n",
    "        data_normalized = self.preprocess(data)\n",
    "        data_normalized = data_normalized\n",
    "        \n",
    "        # Then return the data and its label.\n",
    "        return data_normalized, label\n",
    "        \n",
    "        \n",
    "    # This method should be overrided in order to make it work along with DataLoader class\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load separately VGG16, ResNet, Inception models\n",
    "\n",
    "You can find tutorials on how to load those models at pytorch.org . Don't forget to use pretrained=True if you wish to do finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additionally, build your own model which is different from the other models, train on the Food dataset\n",
    "**Report your loss curves at the end of the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # TO DO: Your neural network design\n",
    "        super(YourModel, self).__init__()\n",
    "        pass\n",
    "        self.seq = nn.Sequential(nn.Linear(10,10))\n",
    "    def forward(self, x):\n",
    "        # TO DO: Your neural network design\n",
    "        out = None\n",
    "        return out\n",
    "\n",
    "model = YourModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###  Here are some training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "regularization_rate = 0\n",
    "n_epochs = 30\n",
    "use_gpu = True\n",
    "test_every = 3\n",
    "###\n",
    "\n",
    "# You may want to tweak them too.\n",
    "optimizer = optim.SGD(params=None, lr=learning_rate)\n",
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Food101Loader(path='drive/blg561/data', mode='train')\n",
    "dataset_test = Food101Loader(path='drive/blg561/data', mode='test')\n",
    "dataset_val = Food101Loader(path='drive/blg561/data', mode='val')\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "test_loader = DataLoader(dataset=dataset_test, batch_size=32, num_workers=4, shuffle=False)\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=32, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finetune/Train the loaded/designed model here:**\n",
    "\n",
    "Don't forget to include appropriate regularizations.\n",
    "Choose appropriate set of hyperparameters such as Learning Rate etc.\n",
    "\n",
    "You may insert new cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It modifies the behaviour of modules like BatchNorm and Dropout for training purposes\n",
    "model.train()\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "    criteria.cuda()\n",
    "\n",
    "# Some example diagnostics.\n",
    "\n",
    "# Loss for every iteration\n",
    "losses_iter_train = []\n",
    "# Loss for epoch (averaging the iteration-wise loss)\n",
    "losses_epoch_train = []\n",
    "accuracy_iter_train = []\n",
    "accuracy_iter_train = []\n",
    "\n",
    "losses_iter_test = []\n",
    "losses_epoch_test = []\n",
    "accuracy_iter_test = []\n",
    "accuracy_iter_test = []\n",
    "\n",
    "# Write the training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for ix, data in train_loader:\n",
    "        model.zero_grad()\n",
    "        img, label = data\n",
    "        if use_gpu:\n",
    "            img.cuda()\n",
    "            label.cuda()\n",
    "        pass\n",
    "        if epoch % 3 == 1:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                # Measure the performance in test set.\n",
    "            \n",
    "            model.train()\n",
    "    \n",
    "        # Fill the rest...\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the performance against validation set\n",
    "Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It modifies the behaviour of modules like BatchNorm and Dropout for test purposes\n",
    "# Dropout no longer works when .eval() is called.\n",
    "# BatchNorm uses the learned parameters\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        if use_gpu:\n",
    "            img.cuda()\n",
    "            label.cuda()\n",
    "        # Fill the rest...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run diagnostics for each model (VGG16, Resnet etc). Avoid overfitting and underfitting as much as possible.\n",
    "We expect you to get at least 75% Test Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot the training, validation, and test losses versus number of iterations or epochs for VGG16 on the same plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot the training, validation, and test losses versus number of iterations or epochs for ResNet on the same plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot the training, validation, and test losses versus number of iterations or epochs for Inception on the same plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Rather than using pretrained weights, first, initialize the weights by using small gaussian with zero mean and unit variance. Secondly, use the Xavier (Glorot) strategy to initialize the weights. Finally, discuss about the differences by comparing number of the epochs required to reach same testing accuracy, loss curves etc.**\n",
    "\n",
    "For more information about Xavier initialization, check out the following link:\n",
    "\n",
    "X. Glorot, Y. Bengio, 2010. Understanding the difficulty of training deep feedforward neural networks:  http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot the training, validation, and test losses versus number of iterations or epochs for your model on the same plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After you have completed the training, save your best model using the following command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 111\n",
    "torch.save(model.state_dict(), 'drive/blg561/{}.pth'.format(student_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
